# ğŸ›ï¸ Retail Analytics with Hadoop & Pig

> A comprehensive big data analytics solution for retail transaction processing and insights generation

[![Hadoop](https://img.shields.io/badge/Hadoop-3.x-orange?logo=apache-hadoop)](https://hadoop.apache.org/)
[![Pig](https://img.shields.io/badge/Apache%20Pig-0.17-blue?logo=apache)](https://pig.apache.org/)
[![Hive](https://img.shields.io/badge/Apache%20Hive-3.x-yellow?logo=apache-hive)](https://hive.apache.org/)

## ğŸ“‹ Overview

This project demonstrates a complete end-to-end big data analytics workflow for retail transaction data using the Hadoop ecosystem. The solution provides comprehensive insights into retail operations through automated data processing and advanced analytics.

### ğŸ¯ Key Features

- **ğŸ“Š Daily & Monthly Sales Analysis** - Track revenue trends over time
- **ğŸ† Top Products Identification** - Discover best-performing items
- **ğŸª Store Performance Metrics** - Compare location effectiveness
- **ğŸ’° Discount Impact Analysis** - Measure promotional effectiveness
- **ğŸ‘¥ RFM Customer Segmentation** - Understand customer behavior patterns

---

## ğŸš€ Quick Start

### Prerequisites

- Hadoop cluster (3.x+)
- Apache Pig (0.17+)
- Apache Hive (3.x+) - Optional
- piggybank.jar for advanced UDFs

---

## ğŸ—‚ï¸ Project Structure

```
/data/retail/
â”œâ”€â”€ raw/                    # Raw CSV files
â”œâ”€â”€ staged/                 # Cleaned & processed data
â”‚   â””â”€â”€ transactions_clean/
â”œâ”€â”€ outputs/               # Analytics results
â”‚   â”œâ”€â”€ total_sales/
â”‚   â”œâ”€â”€ by_product/
â”‚   â”œâ”€â”€ by_store_category/
â”‚   â”œâ”€â”€ discount_analysis/
â”‚   â””â”€â”€ rfm/
â””â”€â”€ archive/               # Historical data

pig-scripts/
â”œâ”€â”€ stage_transactions.pig
â”œâ”€â”€ total_sales.pig
â”œâ”€â”€ top_products.pig
â”œâ”€â”€ store_category.pig
â”œâ”€â”€ discount_analysis.pig
â””â”€â”€ rfm.pig
```

---

## ğŸ”§ Setup Instructions

### 1. ğŸ“ HDFS Directory Setup

```bash
# Create required directories
hdfs dfs -mkdir -p /data/retail/{raw,staged/transactions_clean,outputs,archive}

# Upload transaction data
hdfs dfs -put -f /local/path/transactions.csv /data/retail/raw/

# Verify upload
hdfs dfs -ls /data/retail/raw
```

### 2. ğŸ· Pig Environment Setup

```bash
# Local testing mode
pig -x local script.pig

# Production cluster mode
pig -x mapreduce script.pig \
    -param INPUT=/data/retail/raw/transactions.csv \
    -param OUT=/data/retail/staged/transactions_clean \
    -param PIGGY=/path/to/piggybank.jar
```

---

## ğŸ“ˆ Analytics Pipeline

### Phase 1: ğŸ§¹ Data Staging & Cleaning

The `stage_transactions.pig` script performs comprehensive data preparation:

```bash
pig -x mapreduce stage_transactions.pig \
    -param INPUT=/data/retail/raw/transactions.csv \
    -param OUT=/data/retail/staged/transactions_clean \
    -param PIGGY=/path/to/piggybank.jar
```

**Processing Steps:**
- âœ… CSV parsing with type casting
- âœ‚ï¸ Data trimming and validation  
- ğŸ“… Date standardization (YYYY-MM-DD)
- ğŸ§® Computed fields: GrossAmount, CalcNetAmount
- ğŸš© Validation flags for data quality
- ğŸ’¾ Output as tab-delimited format

### Phase 2: ğŸ“Š Core Analytics

#### ğŸ’¹ Total Sales Analysis
```bash
pig -x mapreduce total_sales.pig \
    -param INPUT=/data/retail/staged/transactions_clean \
    -param OUT=/data/retail/outputs/total_sales
```

#### ğŸ† Top Products Analysis  
```bash
pig -x mapreduce top_products.pig
```

#### ğŸª Store & Category Performance
```bash
pig -x mapreduce store_category.pig
```

#### ğŸ’° Discount Impact Analysis
```bash
pig -x mapreduce discount_analysis.pig
```

#### ğŸ‘¥ RFM Customer Segmentation
```bash
pig -x mapreduce rfm.pig \
    -param INPUT=/data/retail/staged/transactions_clean \
    -param OUT=/data/retail/outputs/rfm \
    -param CURRENT_DATE=2025-09-20
```

---

## ğŸ—ƒï¸ Hive Integration (Optional)

### Create External Table for Staged Data

```sql
CREATE EXTERNAL TABLE IF NOT EXISTS retail_clean (
  CustomerID BIGINT,
  ProductID STRING,
  Quantity INT,
  Price DOUBLE,
  GrossAmount DOUBLE,
  DiscountApplied_pct DOUBLE,
  CalcNetAmount DOUBLE,
  ReportedTotal DOUBLE,
  PaymentMethod STRING,
  StoreLocation STRING,
  ProductCategory STRING,
  txn_date STRING,
  ValidationFlag STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LOCATION '/data/retail/staged/transactions_clean';
```

### Optimize with Parquet Format

```sql
CREATE TABLE IF NOT EXISTS retail_parquet
STORED AS PARQUET
AS SELECT * FROM retail_clean;
```

---

## ğŸ”„ Automation & Orchestration

### Workflow Steps for Airflow/Oozie

1. **ğŸ“¥ Data Ingestion** â†’ Upload CSV files to HDFS
2. **ğŸ§¹ Data Staging** â†’ Execute `stage_transactions.pig`  
3. **ğŸ“Š Analytics Execution** â†’ Run all analytics scripts
4. **ğŸ—ƒï¸ Data Integration** â†’ Import results to Hive
5. **ğŸ“¦ Archival** â†’ Move processed files to archive

---

## ğŸ§ª Testing & Validation

### Local Testing with Sample Data

```bash
# Create sample dataset
head -n 1000 /local/path/transactions.csv > sample.csv

# Test locally
pig -x local stage_transactions.pig \
    -param INPUT=sample.csv \
    -param OUT=/tmp/sample_out \
    -param PIGGY=/path/to/piggybank.jar

# Verify results
hdfs dfs -cat /data/retail/staged/transactions_clean/part-* | head -n 20
```

---

## âš¡ Performance Optimization

### ğŸï¸ Best Practices

- **ğŸ“¦ Compression**: Use Snappy or Deflate for outputs
- **ğŸ“Š Storage Format**: Implement Parquet/ORC for columnar efficiency
- **ğŸ“ Partitioning**: Partition by `txn_date` for query performance
- **ğŸ”— File Management**: Merge small files using `hdfs dfs -getmerge`
- **âš™ï¸ Parallelism**: Tune `default_parallel` setting in Pig
- **ğŸ”§ Custom UDFs**: Register specialized functions for date parsing

### ğŸ“Š Compression Example

```bash
# Enable compression in Pig
pig -Dpig.tmpfilecompression=true \
    -Dpig.tmpfilecompression.codec=gz \
    -x mapreduce script.pig
```

---

## ğŸ” Monitoring & Troubleshooting

### Common Commands

```bash
# Check job progress
yarn application -list

# Monitor HDFS usage
hdfs dfs -du -h /data/retail/

# View Pig logs
tail -f /var/log/pig/pig.log
```

---

## ğŸ“š Resources & Documentation

| Resource | Description | Link |
|----------|-------------|------|
| ğŸ· **Apache Pig** | Official documentation | [pig.apache.org](https://pig.apache.org/) |
| ğŸ§° **Piggybank UDFs** | Built-in user-defined functions | [UDF Documentation](https://pig.apache.org/docs/r0.17.0/udf.html) |
| ğŸ—ƒï¸ **Hadoop HDFS** | File system guide | [HDFS User Guide](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html) |
| ğŸ›ï¸ **Apache Hive** | External tables & Parquet | [Hive Language Manual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL) |

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™‹â€â™€ï¸ Support

For questions and support:
- ğŸ“§ Email: [support@company.com](mailto:support@company.com)
- ğŸ“ Issues: [GitHub Issues](https://github.com/yourorg/retail-analytics/issues)
- ğŸ’¬ Discussions: [GitHub Discussions](https://github.com/yourorg/retail-analytics/discussions)

---

<div align="center">

**Built with â¤ï¸ using the Hadoop Ecosystem**

[â­ Star this repo](https://github.com/yourorg/retail-analytics) | [ğŸ´ Fork it](https://github.com/yourorg/retail-analytics/fork) | [ğŸ“ Report Issues](https://github.com/yourorg/retail-analytics/issues)

</div>