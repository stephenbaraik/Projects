{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24457b92",
   "metadata": {},
   "source": [
    "# ðŸ›¢ï¸ Crude Oil & Nifty50 Forecasting Analysis\n",
    "\n",
    "## ðŸ“Š Comprehensive Analysis of Oil Price Impact on Indian Stock Market\n",
    "\n",
    "This notebook analyzes the relationship between Brent crude oil prices and the Nifty50 index to forecast stock market movements based on oil price changes.\n",
    "\n",
    "### ðŸŽ¯ Analysis Objectives:\n",
    "- Collect and preprocess historical data for Brent crude oil and Nifty50\n",
    "- Engineer relevant features including technical indicators and lagged variables\n",
    "- Train multiple machine learning models to predict Nifty50 returns\n",
    "- Evaluate model performance and identify key predictive features\n",
    "- Conduct scenario analysis for different oil price change scenarios\n",
    "\n",
    "### ðŸ“ˆ Key Findings from Previous Analysis:\n",
    "- **Model Performance**: Negative RÂ² scores indicate challenging prediction task\n",
    "- **Feature Correlations**: Low correlations suggest weak direct relationships\n",
    "- **Data Quality**: Clean dataset with 3,605 records after preprocessing\n",
    "- **Scenario Results**: Counterintuitive relationships require further investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbcec58",
   "metadata": {},
   "source": [
    "## 1. ðŸ“š Import Required Libraries\n",
    "\n",
    "Setting up the analytical environment with all necessary libraries for data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6dbfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# Financial data\n",
    "import yfinance as yf\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"ðŸ“Š Pandas version: {pd.__version__}\")\n",
    "print(f\"ðŸ§® NumPy version: {np.__version__}\")\n",
    "print(f\"ðŸ¤– Scikit-learn available\")\n",
    "print(f\"ðŸ“ˆ XGBoost available\")\n",
    "print(f\"ðŸ’° YFinance available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664c2c08",
   "metadata": {},
   "source": [
    "## 2. ðŸ“¦ Data Collection and Setup\n",
    "\n",
    "Fetching historical data for Brent crude oil and Nifty50 index from Yahoo Finance. We'll collect data from 2010 to present to ensure sufficient historical context for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407939bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(start_date=\"2010-01-01\", end_date=\"2025-06-30\"):\n",
    "    \"\"\"\n",
    "    ðŸ“¦ Data Collection: Fetch Brent crude oil and Nifty50 data\n",
    "    \"\"\"\n",
    "    print(\"ðŸ“¦ Collecting data...\")\n",
    "    \n",
    "    # Fetch Brent crude oil data (BZ=F is Brent crude futures)\n",
    "    print(\"Fetching Brent crude oil data...\")\n",
    "    brent = yf.download(\"BZ=F\", start=start_date, end=end_date, progress=False)\n",
    "    brent = brent[['Close']].rename(columns={'Close': 'brent_price'})\n",
    "    \n",
    "    # Fetch Nifty50 data (^NSEI is Nifty50 index)\n",
    "    print(\"Fetching Nifty50 data...\")\n",
    "    nifty = yf.download(\"^NSEI\", start=start_date, end=end_date, progress=False)\n",
    "    nifty = nifty[['Close']].rename(columns={'Close': 'nifty_price'})\n",
    "    \n",
    "    # Convert Brent from USD to INR (approximate conversion rate)\n",
    "    # For simplicity, using a fixed rate - in practice, you'd fetch USD/INR data\n",
    "    usd_to_inr = 83.0  # Approximate current rate\n",
    "    brent['brent_price_inr'] = brent['brent_price'] * usd_to_inr\n",
    "    \n",
    "    # Merge datasets on date\n",
    "    data = pd.merge(brent, nifty, left_index=True, right_index=True, how='inner')\n",
    "    data = data.dropna()\n",
    "    \n",
    "    print(f\"âœ… Data collected: {len(data)} records from {data.index[0].date()} to {data.index[-1].date()}\")\n",
    "    \n",
    "    # Display basic information\n",
    "    print(f\"\\nðŸ“Š Dataset Overview:\")\n",
    "    print(f\"   â€¢ Brent Price Range: ${data['brent_price'].min():.2f} - ${data['brent_price'].max():.2f}\")\n",
    "    print(f\"   â€¢ Brent Price (INR) Range: â‚¹{data['brent_price_inr'].min():.2f} - â‚¹{data['brent_price_inr'].max():.2f}\")\n",
    "    print(f\"   â€¢ Nifty50 Range: {data['nifty_price'].min():.2f} - {data['nifty_price'].max():.2f}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Collect the data\n",
    "raw_data = collect_data()\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nðŸ“‹ First 5 rows of the dataset:\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e73a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the raw data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Brent crude oil price in USD\n",
    "axes[0, 0].plot(raw_data.index, raw_data['brent_price'], color='brown', linewidth=1)\n",
    "axes[0, 0].set_title('Brent Crude Oil Price (USD)')\n",
    "axes[0, 0].set_ylabel('Price (USD)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Brent crude oil price in INR\n",
    "axes[0, 1].plot(raw_data.index, raw_data['brent_price_inr'], color='orange', linewidth=1)\n",
    "axes[0, 1].set_title('Brent Crude Oil Price (INR)')\n",
    "axes[0, 1].set_ylabel('Price (INR)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Nifty50 index\n",
    "axes[1, 0].plot(raw_data.index, raw_data['nifty_price'], color='blue', linewidth=1)\n",
    "axes[1, 0].set_title('Nifty50 Index')\n",
    "axes[1, 0].set_ylabel('Index Value')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Correlation heatmap\n",
    "correlation_matrix = raw_data[['brent_price', 'brent_price_inr', 'nifty_price']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Basic Statistics:\")\n",
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e656a8",
   "metadata": {},
   "source": [
    "## 3. ðŸ§¹ Data Preprocessing and Feature Engineering\n",
    "\n",
    "Creating meaningful features from raw price data including:\n",
    "- **Daily percentage changes** for both oil and Nifty\n",
    "- **Rolling averages and volatility** measures\n",
    "- **Lagged features** to capture temporal relationships\n",
    "- **Momentum indicators** and technical analysis features\n",
    "- **Target variables** for prediction (30-day forward returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e5d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    ðŸ§¹ Data Preprocessing: Feature engineering and target creation\n",
    "    \"\"\"\n",
    "    print(\"ðŸ§¹ Preprocessing data...\")\n",
    "    \n",
    "    df = data.copy()\n",
    "    \n",
    "    # Calculate daily percent changes\n",
    "    df['brent_pct_change'] = df['brent_price_inr'].pct_change()\n",
    "    df['nifty_pct_change'] = df['nifty_price'].pct_change()\n",
    "    \n",
    "    # Rolling averages and volatility\n",
    "    for window in [7, 30]:\n",
    "        df[f'brent_rolling_mean_{window}'] = df['brent_price_inr'].rolling(window=window).mean()\n",
    "        df[f'brent_rolling_std_{window}'] = df['brent_price_inr'].rolling(window=window).std()\n",
    "    \n",
    "    # Lagged features\n",
    "    for lag in [1, 3, 5, 7, 14]:\n",
    "        df[f'brent_lag_{lag}'] = df['brent_pct_change'].shift(lag)\n",
    "        df[f'nifty_lag_{lag}'] = df['nifty_pct_change'].shift(lag)\n",
    "    \n",
    "    # Target variables\n",
    "    df['nifty_return_next_day'] = df['nifty_price'].pct_change().shift(-1)\n",
    "    df['nifty_return_30d'] = (df['nifty_price'].shift(-30) - df['nifty_price']) / df['nifty_price']\n",
    "    \n",
    "    # Additional features\n",
    "    df['brent_momentum_7'] = df['brent_price_inr'] / df['brent_rolling_mean_7']\n",
    "    df['brent_momentum_30'] = df['brent_price_inr'] / df['brent_rolling_mean_30']\n",
    "    df['volatility_ratio'] = df['brent_rolling_std_7'] / df['brent_rolling_std_30']\n",
    "    \n",
    "    # Price ratios and technical indicators\n",
    "    df['brent_price_ratio'] = df['brent_price_inr'] / df['brent_price_inr'].shift(1)\n",
    "    df['nifty_price_ratio'] = df['nifty_price'] / df['nifty_price'].shift(1)\n",
    "    \n",
    "    # Moving average convergence divergence (MACD) style indicator\n",
    "    df['brent_ema_12'] = df['brent_price_inr'].ewm(span=12).mean()\n",
    "    df['brent_ema_26'] = df['brent_price_inr'].ewm(span=26).mean()\n",
    "    df['brent_macd'] = df['brent_ema_12'] - df['brent_ema_26']\n",
    "    \n",
    "    # Remove rows with NaN values\n",
    "    df_cleaned = df.dropna()\n",
    "    \n",
    "    print(f\"âœ… Preprocessing complete: {len(df_cleaned)} records after cleaning\")\n",
    "    print(f\"ðŸ“‰ Removed {len(df) - len(df_cleaned)} rows due to NaN values\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# Preprocess the data\n",
    "processed_data = preprocess_data(raw_data)\n",
    "\n",
    "# Display information about created features\n",
    "feature_cols = [col for col in processed_data.columns if col not in ['brent_price', 'nifty_price', 'brent_price_inr']]\n",
    "print(f\"\\nðŸ”§ Created Features ({len(feature_cols)} total):\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"   {i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Processed Data Shape: {processed_data.shape}\")\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key engineered features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Daily percentage changes\n",
    "axes[0, 0].plot(processed_data.index, processed_data['brent_pct_change'], alpha=0.7, label='Brent % Change')\n",
    "axes[0, 0].plot(processed_data.index, processed_data['nifty_pct_change'], alpha=0.7, label='Nifty % Change')\n",
    "axes[0, 0].set_title('Daily Percentage Changes')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Rolling volatility\n",
    "axes[0, 1].plot(processed_data.index, processed_data['brent_rolling_std_7'], label='7-day Volatility')\n",
    "axes[0, 1].plot(processed_data.index, processed_data['brent_rolling_std_30'], label='30-day Volatility')\n",
    "axes[0, 1].set_title('Brent Oil Volatility')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Momentum indicators\n",
    "axes[0, 2].plot(processed_data.index, processed_data['brent_momentum_7'], label='7-day Momentum')\n",
    "axes[0, 2].plot(processed_data.index, processed_data['brent_momentum_30'], label='30-day Momentum')\n",
    "axes[0, 2].axhline(y=1, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0, 2].set_title('Brent Oil Momentum')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Target variable distribution\n",
    "axes[1, 0].hist(processed_data['nifty_return_30d'], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Distribution of 30-day Nifty Returns')\n",
    "axes[1, 0].set_xlabel('30-day Return')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# MACD indicator\n",
    "axes[1, 1].plot(processed_data.index, processed_data['brent_macd'], color='purple', alpha=0.7)\n",
    "axes[1, 1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].set_title('Brent Oil MACD')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Correlation between oil and nifty changes\n",
    "axes[1, 2].scatter(processed_data['brent_pct_change'], processed_data['nifty_pct_change'], alpha=0.5)\n",
    "axes[1, 2].set_xlabel('Brent % Change')\n",
    "axes[1, 2].set_ylabel('Nifty % Change')\n",
    "axes[1, 2].set_title('Daily Changes Correlation')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display correlation\n",
    "correlation = processed_data['brent_pct_change'].corr(processed_data['nifty_pct_change'])\n",
    "print(f\"\\nðŸ“Š Daily Changes Correlation: {correlation:.4f}\")\n",
    "\n",
    "# Display target variable statistics\n",
    "print(f\"\\nðŸŽ¯ Target Variable (30-day Nifty Returns) Statistics:\")\n",
    "print(f\"   â€¢ Mean: {processed_data['nifty_return_30d'].mean():.4f} ({processed_data['nifty_return_30d'].mean()*100:.2f}%)\")\n",
    "print(f\"   â€¢ Std: {processed_data['nifty_return_30d'].std():.4f} ({processed_data['nifty_return_30d'].std()*100:.2f}%)\")\n",
    "print(f\"   â€¢ Min: {processed_data['nifty_return_30d'].min():.4f} ({processed_data['nifty_return_30d'].min()*100:.2f}%)\")\n",
    "print(f\"   â€¢ Max: {processed_data['nifty_return_30d'].max():.4f} ({processed_data['nifty_return_30d'].max()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106f179f",
   "metadata": {},
   "source": [
    "## 4. ðŸŽ¯ Feature Preparation and Train-Test Split\n",
    "\n",
    "Selecting the most relevant features for our prediction model and splitting the data into training and testing sets while maintaining temporal order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e483feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(data, target='nifty_return_30d'):\n",
    "    \"\"\"\n",
    "    Prepare feature matrix and target variable\n",
    "    \"\"\"\n",
    "    # Select features that might be predictive\n",
    "    feature_cols = [\n",
    "        'brent_pct_change', 'brent_rolling_mean_7', 'brent_rolling_std_7',\n",
    "        'brent_lag_1', 'brent_lag_3', 'brent_lag_7', 'brent_lag_14',\n",
    "        'brent_momentum_7', 'brent_momentum_30', 'volatility_ratio',\n",
    "        'nifty_lag_1', 'nifty_lag_3', 'nifty_lag_7',\n",
    "        'brent_macd', 'brent_price_ratio'\n",
    "    ]\n",
    "    \n",
    "    # Ensure all features exist in the data\n",
    "    available_features = [col for col in feature_cols if col in data.columns]\n",
    "    if len(available_features) != len(feature_cols):\n",
    "        missing = set(feature_cols) - set(available_features)\n",
    "        print(f\"âš ï¸ Missing features: {missing}\")\n",
    "    \n",
    "    X = data[available_features].copy()\n",
    "    y = data[target].copy()\n",
    "    \n",
    "    # Remove any remaining NaN values\n",
    "    mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    \n",
    "    # Train-test split (80-20, with most recent data as test)\n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train = X.iloc[:split_idx]\n",
    "    X_test = X.iloc[split_idx:]\n",
    "    y_train = y.iloc[:split_idx]\n",
    "    y_test = y.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"ðŸ“Š Feature Selection Complete:\")\n",
    "    print(f\"   â€¢ Selected Features: {len(available_features)}\")\n",
    "    print(f\"   â€¢ Total Samples: {len(X)}\")\n",
    "    print(f\"   â€¢ Train Set: {len(X_train)} samples ({split_idx/len(X)*100:.1f}%)\")\n",
    "    print(f\"   â€¢ Test Set: {len(X_test)} samples ({(len(X)-split_idx)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   â€¢ Train Period: {X_train.index[0].date()} to {X_train.index[-1].date()}\")\n",
    "    print(f\"   â€¢ Test Period: {X_test.index[0].date()} to {X_test.index[-1].date()}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, available_features\n",
    "\n",
    "# Prepare features and split data\n",
    "X_train, X_test, y_train, y_test, feature_names = prepare_features(processed_data)\n",
    "\n",
    "# Display feature information\n",
    "print(f\"\\nðŸ”§ Selected Features for Modeling:\")\n",
    "for i, feature in enumerate(feature_names, 1):\n",
    "    print(f\"   {i:2d}. {feature}\")\n",
    "\n",
    "# Quick feature correlation with target\n",
    "feature_target_corr = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Correlation': [abs(X_train[feature].corr(y_train)) for feature in feature_names]\n",
    "}).sort_values('Correlation', ascending=False)\n",
    "\n",
    "print(f\"\\nðŸ“Š Feature-Target Correlations (Top 5):\")\n",
    "for _, row in feature_target_corr.head().iterrows():\n",
    "    print(f\"   â€¢ {row['Feature']}: {row['Correlation']:.4f}\")\n",
    "    \n",
    "print(f\"\\nðŸ“ˆ Feature Matrix Shape: {X_train.shape}\")\n",
    "print(f\"ðŸŽ¯ Target Vector Shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d78be36",
   "metadata": {},
   "source": [
    "## 5. ðŸ§  Model Training - Basic Models\n",
    "\n",
    "Training our first set of machine learning models with default parameters to establish baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d8f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_basic_models(X_train, y_train):\n",
    "    \"\"\"\n",
    "    ðŸ§  Train basic machine learning models\n",
    "    \"\"\"\n",
    "    print(\"ðŸ§  Training basic models...\")\n",
    "    \n",
    "    models = {}\n",
    "    feature_importance = {}\n",
    "    \n",
    "    # Linear Regression\n",
    "    print(\"   Training Linear Regression...\")\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    models['Linear Regression'] = lr\n",
    "    \n",
    "    # Random Forest\n",
    "    print(\"   Training Random Forest...\")\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    models['Random Forest'] = rf\n",
    "    feature_importance['Random Forest'] = pd.Series(\n",
    "        rf.feature_importances_, index=X_train.columns\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    # XGBoost\n",
    "    print(\"   Training XGBoost...\")\n",
    "    xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    models['XGBoost'] = xgb_model\n",
    "    feature_importance['XGBoost'] = pd.Series(\n",
    "        xgb_model.feature_importances_, index=X_train.columns\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    print(\"âœ… Basic models trained successfully!\")\n",
    "    \n",
    "    return models, feature_importance\n",
    "\n",
    "# Train basic models\n",
    "basic_models, basic_feature_importance = train_basic_models(X_train, y_train)\n",
    "\n",
    "# Display feature importance for Random Forest\n",
    "print(\"\\\\nðŸŒ² Random Forest Feature Importance (Top 10):\")\n",
    "for feature, importance in basic_feature_importance['Random Forest'].head(10).items():\n",
    "    print(f\"   â€¢ {feature}: {importance:.4f}\")\n",
    "\n",
    "print(\"\\\\nâš¡ XGBoost Feature Importance (Top 10):\")\n",
    "for feature, importance in basic_feature_importance['XGBoost'].head(10).items():\n",
    "    print(f\"   â€¢ {feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f909a",
   "metadata": {},
   "source": [
    "## 6. ðŸš€ Model Training - Improved Models\n",
    "\n",
    "Training enhanced versions of our models with feature scaling, regularization, and optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_improved_models(X_train, y_train):\n",
    "    \"\"\"\n",
    "    ðŸš€ Train improved models with better feature engineering\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ Training improved models...\")\n",
    "    \n",
    "    models = {}\n",
    "    feature_importance = {}\n",
    "    \n",
    "    # Feature scaling for better performance\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Ridge Regression with regularization\n",
    "    print(\"   Training Ridge Regression...\")\n",
    "    ridge = Ridge(alpha=1.0)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    models['Ridge Regression'] = ridge\n",
    "    models['scaler'] = scaler  # Store scaler for later use\n",
    "    \n",
    "    # Random Forest with better hyperparameters\n",
    "    print(\"   Training Improved Random Forest...\")\n",
    "    rf_improved = RandomForestRegressor(\n",
    "        n_estimators=200, \n",
    "        max_depth=10,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=10,\n",
    "        random_state=42, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_improved.fit(X_train, y_train)\n",
    "    models['Random Forest (Improved)'] = rf_improved\n",
    "    feature_importance['Random Forest (Improved)'] = pd.Series(\n",
    "        rf_improved.feature_importances_, index=X_train.columns\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    # XGBoost with better hyperparameters\n",
    "    print(\"   Training Improved XGBoost...\")\n",
    "    xgb_improved = xgb.XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    xgb_improved.fit(X_train, y_train)\n",
    "    models['XGBoost (Improved)'] = xgb_improved\n",
    "    feature_importance['XGBoost (Improved)'] = pd.Series(\n",
    "        xgb_improved.feature_importances_, index=X_train.columns\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    print(\"âœ… Improved models trained successfully!\")\n",
    "    \n",
    "    return models, feature_importance\n",
    "\n",
    "# Train improved models\n",
    "improved_models, improved_feature_importance = train_improved_models(X_train, y_train)\n",
    "\n",
    "# Combine all models\n",
    "all_models = {**basic_models, **improved_models}\n",
    "all_feature_importance = {**basic_feature_importance, **improved_feature_importance}\n",
    "\n",
    "print(f\"\\\\nðŸŽ¯ Total Models Trained: {len(all_models) - 1}\")  # -1 for scaler\n",
    "for model_name in all_models.keys():\n",
    "    if model_name != 'scaler':\n",
    "        print(f\"   â€¢ {model_name}\")\n",
    "\n",
    "print(\"\\\\nðŸš€ Improved XGBoost Feature Importance (Top 10):\")\n",
    "for feature, importance in improved_feature_importance['XGBoost (Improved)'].head(10).items():\n",
    "    print(f\"   â€¢ {feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7504df72",
   "metadata": {},
   "source": [
    "## 7. ðŸ“Š Model Evaluation and Performance Metrics\n",
    "\n",
    "Evaluating all trained models using comprehensive metrics including MAE, RMSE, R-squared, and directional accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2052400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X_test, y_test):\n",
    "    \"\"\"\n",
    "    ðŸ“Š Evaluate model performance comprehensively\n",
    "    \"\"\"\n",
    "    print(\"ðŸ“Š Evaluating models...\")\n",
    "    \n",
    "    results = {}\n",
    "    predictions = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        if name == 'scaler':\n",
    "            continue\n",
    "            \n",
    "        print(f\"   Evaluating {name}...\")\n",
    "        \n",
    "        # Make predictions\n",
    "        if name == 'Ridge Regression':\n",
    "            # Use scaled features for Ridge regression\n",
    "            y_pred = model.predict(models['scaler'].transform(X_test))\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "        predictions[name] = y_pred\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Directional accuracy\n",
    "        actual_direction = (y_test > 0).astype(int)\n",
    "        pred_direction = (y_pred > 0).astype(int)\n",
    "        directional_accuracy = (actual_direction == pred_direction).mean()\n",
    "        \n",
    "        # Additional metrics\n",
    "        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "        \n",
    "        results[name] = {\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'RÂ²': r2,\n",
    "            'Directional Accuracy': directional_accuracy,\n",
    "            'MAPE': mape\n",
    "        }\n",
    "    \n",
    "    return results, predictions\n",
    "\n",
    "# Evaluate all models\n",
    "model_results, model_predictions = evaluate_models(all_models, X_test, y_test)\n",
    "\n",
    "# Create results DataFrame for better visualization\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(\"\\\\nðŸ“ˆ Model Performance Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Find best model for each metric\n",
    "print(\"\\\\nðŸ† Best Performing Models:\")\n",
    "for metric in ['MAE', 'RMSE', 'RÂ²', 'Directional Accuracy']:\n",
    "    if metric == 'RÂ²' or metric == 'Directional Accuracy':\n",
    "        best_model = results_df[metric].idxmax()\n",
    "        best_value = results_df[metric].max()\n",
    "    else:\n",
    "        best_model = results_df[metric].idxmin()\n",
    "        best_value = results_df[metric].min()\n",
    "    \n",
    "    print(f\"   â€¢ {metric}: {best_model} ({best_value:.4f})\")\n",
    "\n",
    "# Calculate baseline performance\n",
    "baseline_pred = np.full_like(y_test, y_train.mean())\n",
    "baseline_r2 = r2_score(y_test, baseline_pred)\n",
    "baseline_mae = mean_absolute_error(y_test, baseline_pred)\n",
    "\n",
    "print(f\"\\\\nðŸ“Š Baseline Performance (Mean Prediction):\")\n",
    "print(f\"   â€¢ RÂ²: {baseline_r2:.4f}\")\n",
    "print(f\"   â€¢ MAE: {baseline_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f255c696",
   "metadata": {},
   "source": [
    "## 8. ðŸ“ˆ Results Visualization\n",
    "\n",
    "Comprehensive visualizations to understand model performance, feature importance, and prediction quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Actual vs Predicted scatter plot\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']\n",
    "for i, (name, pred) in enumerate(model_predictions.items()):\n",
    "    plt.scatter(y_test, pred, alpha=0.6, label=name, color=colors[i % len(colors)])\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual 30-day Returns')\n",
    "plt.ylabel('Predicted 30-day Returns')\n",
    "plt.title('Actual vs Predicted Returns')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Model performance comparison (MAE and RMSE)\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "metrics_subset = results_df[['MAE', 'RMSE']]\n",
    "metrics_subset.plot(kind='bar', ax=ax2, color=['skyblue', 'lightcoral'])\n",
    "plt.title('Model Performance: MAE & RMSE')\n",
    "plt.ylabel('Error')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# 3. RÂ² and Directional Accuracy\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "performance_metrics = results_df[['RÂ²', 'Directional Accuracy']]\n",
    "performance_metrics.plot(kind='bar', ax=ax3, color=['lightgreen', 'gold'])\n",
    "plt.title('Model Performance: RÂ² & Directional Accuracy')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# 4. Feature importance (XGBoost Improved)\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "if 'XGBoost (Improved)' in all_feature_importance:\n",
    "    feat_imp = all_feature_importance['XGBoost (Improved)'].head(10)\n",
    "    feat_imp.plot(kind='barh', ax=ax4, color='steelblue')\n",
    "    plt.title('Feature Importance (XGBoost Improved)')\n",
    "    plt.xlabel('Importance')\n",
    "\n",
    "# 5. Time series of predictions vs actual\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "test_dates = y_test.index\n",
    "plt.plot(test_dates, y_test, label='Actual', linewidth=2, color='black')\n",
    "for i, (name, pred) in enumerate(list(model_predictions.items())[:3]):  # Show top 3 models\n",
    "    plt.plot(test_dates, pred, label=f'{name}', alpha=0.8, color=colors[i])\n",
    "plt.title('Time Series: Actual vs Predicted (Top 3 Models)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('30-day Returns')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 6. Residuals plot for best model (by RÂ²)\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "best_model_name = results_df['RÂ²'].idxmax()\n",
    "best_predictions = model_predictions[best_model_name]\n",
    "residuals = y_test - best_predictions\n",
    "plt.scatter(best_predictions, residuals, alpha=0.6, color='purple')\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title(f'Residuals Plot: {best_model_name}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Distribution of actual vs predicted returns\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "plt.hist(y_test, bins=30, alpha=0.7, label='Actual', color='blue', density=True)\n",
    "plt.hist(best_predictions, bins=30, alpha=0.7, label=f'Predicted ({best_model_name})', color='red', density=True)\n",
    "plt.xlabel('30-day Returns')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution Comparison')\n",
    "plt.legend()\n",
    "\n",
    "# 8. Error distribution\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "errors = np.abs(y_test - best_predictions)\n",
    "plt.hist(errors, bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "plt.xlabel('Absolute Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Error Distribution: {best_model_name}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Correlation matrix of predictions\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "pred_df = pd.DataFrame(model_predictions)\n",
    "pred_corr = pred_df.corr()\n",
    "sns.heatmap(pred_corr, annot=True, cmap='coolwarm', center=0, ax=ax9, \n",
    "            square=True, cbar_kws={'shrink': 0.8})\n",
    "plt.title('Model Predictions Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\\\nðŸ“Š Detailed Analysis Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ðŸŽ¯ Best Overall Model (by RÂ²): {results_df['RÂ²'].idxmax()}\")\n",
    "print(f\"ðŸ“ˆ Best RÂ² Score: {results_df['RÂ²'].max():.4f}\")\n",
    "print(f\"ðŸŽ¯ Best Directional Accuracy: {results_df['Directional Accuracy'].idxmax()}\")\n",
    "print(f\"ðŸ“ˆ Best Direction Score: {results_df['Directional Accuracy'].max():.4f}\")\n",
    "\n",
    "# Calculate prediction ranges\n",
    "print(f\"\\\\nðŸ“Š Prediction Analysis:\")\n",
    "for name, pred in model_predictions.items():\n",
    "    pred_range = pred.max() - pred.min()\n",
    "    print(f\"   â€¢ {name}: Range {pred_range:.4f} (Min: {pred.min():.4f}, Max: {pred.max():.4f})\")\n",
    "\n",
    "actual_range = y_test.max() - y_test.min()\n",
    "print(f\"   â€¢ Actual: Range {actual_range:.4f} (Min: {y_test.min():.4f}, Max: {y_test.max():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f14588",
   "metadata": {},
   "source": [
    "## 9. ðŸ” Scenario Analysis\n",
    "\n",
    "Testing how different oil price change scenarios impact Nifty50 predictions using our best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario_analysis(data, models, feature_names, brent_change_pct=10, model_name=None):\n",
    "    \"\"\"\n",
    "    ðŸ” Scenario Analysis: What if Brent oil changes by X%?\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = results_df['RÂ²'].idxmax()  # Use best model by RÂ²\n",
    "    \n",
    "    print(f\"ðŸ” Scenario Analysis: Brent oil {brent_change_pct:+.1f}% change impact\")\n",
    "    print(f\"ðŸ“Š Using Model: {model_name}\")\n",
    "    \n",
    "    # Get the latest data point\n",
    "    latest_data = data.iloc[-1:].copy()\n",
    "    \n",
    "    # Create scenario where Brent changes by specified percentage\n",
    "    scenario_data = latest_data.copy()\n",
    "    \n",
    "    # Update features based on the oil price change\n",
    "    daily_change = brent_change_pct / 100  # Convert to decimal\n",
    "    scenario_data['brent_pct_change'] = daily_change\n",
    "    \n",
    "    # Update momentum and other derived features\n",
    "    scenario_data['brent_momentum_7'] = 1 + (brent_change_pct / 100)\n",
    "    scenario_data['brent_momentum_30'] = 1 + (brent_change_pct / 200)  # Less impact on 30-day\n",
    "    \n",
    "    # Adjust volatility (oil price changes often increase volatility)\n",
    "    if 'volatility_ratio' in scenario_data.columns:\n",
    "        scenario_data['volatility_ratio'] = scenario_data['volatility_ratio'] * (1 + abs(brent_change_pct) / 100)\n",
    "    \n",
    "    # Update MACD if available\n",
    "    if 'brent_macd' in scenario_data.columns:\n",
    "        scenario_data['brent_macd'] = scenario_data['brent_macd'] * (1 + brent_change_pct / 100)\n",
    "    \n",
    "    # Update price ratio\n",
    "    if 'brent_price_ratio' in scenario_data.columns:\n",
    "        scenario_data['brent_price_ratio'] = 1 + (brent_change_pct / 100)\n",
    "    \n",
    "    # Make prediction\n",
    "    model = models[model_name]\n",
    "    features = scenario_data[feature_names]\n",
    "    \n",
    "    if model_name == 'Ridge Regression':\n",
    "        predicted_return = model.predict(models['scaler'].transform(features))[0]\n",
    "    else:\n",
    "        predicted_return = model.predict(features)[0]\n",
    "    \n",
    "    # Calculate predicted price\n",
    "    current_nifty = float(data['nifty_price'].iloc[-1])\n",
    "    predicted_price = current_nifty * (1 + predicted_return)\n",
    "    \n",
    "    print(f\"\\\\nðŸ“Š Scenario Results:\")\n",
    "    print(f\"   â€¢ Current Nifty50: {current_nifty:.2f}\")\n",
    "    print(f\"   â€¢ Predicted 30-day return: {predicted_return:.4f} ({predicted_return*100:.2f}%)\")\n",
    "    print(f\"   â€¢ Predicted Nifty50 price: {predicted_price:.2f}\")\n",
    "    print(f\"   â€¢ Expected price change: {predicted_price - current_nifty:.2f}\")\n",
    "    print(f\"   â€¢ Impact ratio: {(predicted_return * 100) / brent_change_pct:.2f}% Nifty change per 1% oil change\")\n",
    "    \n",
    "    return predicted_return, predicted_price\n",
    "\n",
    "# Define scenario test cases\n",
    "scenarios = [-20, -15, -10, -5, 0, 5, 10, 15, 20, 25]\n",
    "scenario_results = []\n",
    "\n",
    "print(\"ðŸ”® Comprehensive Scenario Testing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_model = results_df['RÂ²'].idxmax()\n",
    "print(f\"Using best model: {best_model} (RÂ² = {results_df.loc[best_model, 'RÂ²']:.4f})\\\\n\")\n",
    "\n",
    "for change in scenarios:\n",
    "    if change == 0:\n",
    "        print(f\"ðŸ” Baseline Scenario: No oil price change\")\n",
    "        print(f\"\\\\nðŸ“Š Scenario Results:\")\n",
    "        current_nifty = float(processed_data['nifty_price'].iloc[-1])\n",
    "        print(f\"   â€¢ Current Nifty50: {current_nifty:.2f}\")\n",
    "        print(f\"   â€¢ Predicted 30-day return: 0.0000 (0.00%)\")\n",
    "        print(f\"   â€¢ Predicted Nifty50 price: {current_nifty:.2f}\")\n",
    "        print(f\"   â€¢ Expected price change: 0.00\")\n",
    "        print(f\"   â€¢ Impact ratio: 0.00% Nifty change per 1% oil change\")\n",
    "        scenario_results.append((change, 0.0, current_nifty))\n",
    "    else:\n",
    "        predicted_return, predicted_price = scenario_analysis(\n",
    "            processed_data, all_models, feature_names, \n",
    "            brent_change_pct=change, model_name=best_model\n",
    "        )\n",
    "        scenario_results.append((change, predicted_return, predicted_price))\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Visualize scenario results\n",
    "scenario_df = pd.DataFrame(scenario_results, columns=['Oil_Change_%', 'Predicted_Return', 'Predicted_Price'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Oil change vs Nifty return\n",
    "axes[0].plot(scenario_df['Oil_Change_%'], scenario_df['Predicted_Return'] * 100, \n",
    "             marker='o', linewidth=2, markersize=8, color='blue')\n",
    "axes[0].set_xlabel('Oil Price Change (%)')\n",
    "axes[0].set_ylabel('Predicted Nifty Return (%)')\n",
    "axes[0].set_title('Oil Price Change vs Nifty Return')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Oil change vs Nifty price\n",
    "current_price = float(processed_data['nifty_price'].iloc[-1])\n",
    "axes[1].plot(scenario_df['Oil_Change_%'], scenario_df['Predicted_Price'], \n",
    "             marker='s', linewidth=2, markersize=8, color='green')\n",
    "axes[1].axhline(y=current_price, color='red', linestyle='--', alpha=0.5, label=f'Current: {current_price:.0f}')\n",
    "axes[1].set_xlabel('Oil Price Change (%)')\n",
    "axes[1].set_ylabel('Predicted Nifty Price')\n",
    "axes[1].set_title('Oil Price Change vs Nifty Price')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "# Impact sensitivity\n",
    "impact_ratio = []\n",
    "for i, row in scenario_df.iterrows():\n",
    "    if row['Oil_Change_%'] != 0:\n",
    "        ratio = (row['Predicted_Return'] * 100) / row['Oil_Change_%']\n",
    "        impact_ratio.append((row['Oil_Change_%'], ratio))\n",
    "\n",
    "if impact_ratio:\n",
    "    impact_df = pd.DataFrame(impact_ratio, columns=['Oil_Change_%', 'Impact_Ratio'])\n",
    "    axes[2].plot(impact_df['Oil_Change_%'], impact_df['Impact_Ratio'], \n",
    "                 marker='^', linewidth=2, markersize=8, color='purple')\n",
    "    axes[2].set_xlabel('Oil Price Change (%)')\n",
    "    axes[2].set_ylabel('Nifty Response per 1% Oil Change (%)')\n",
    "    axes[2].set_title('Impact Sensitivity Analysis')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary insights\n",
    "print(\"\\\\nðŸ“Š Scenario Analysis Insights:\")\n",
    "max_positive_impact = scenario_df[scenario_df['Oil_Change_%'] > 0]['Predicted_Return'].max()\n",
    "max_negative_impact = scenario_df[scenario_df['Oil_Change_%'] < 0]['Predicted_Return'].min()\n",
    "\n",
    "print(f\"   â€¢ Largest positive Nifty impact: {max_positive_impact*100:.2f}% for oil price increase\")\n",
    "print(f\"   â€¢ Largest negative Nifty impact: {max_negative_impact*100:.2f}% for oil price decrease\")\n",
    "\n",
    "if impact_ratio:\n",
    "    avg_sensitivity = np.mean([abs(ratio[1]) for ratio in impact_ratio])\n",
    "    print(f\"   â€¢ Average sensitivity: {avg_sensitivity:.2f}% Nifty change per 1% oil change\")\n",
    "    \n",
    "print(f\"   â€¢ Current Nifty50 level: {current_price:.2f}\")\n",
    "print(f\"   â€¢ Potential price range: {scenario_df['Predicted_Price'].min():.2f} - {scenario_df['Predicted_Price'].max():.2f}\")\n",
    "\n",
    "# Display scenario summary table\n",
    "print(\"\\\\nðŸ“‹ Scenario Summary Table:\")\n",
    "scenario_display = scenario_df.copy()\n",
    "scenario_display['Predicted_Return_%'] = (scenario_display['Predicted_Return'] * 100).round(2)\n",
    "scenario_display['Price_Change'] = (scenario_display['Predicted_Price'] - current_price).round(2)\n",
    "scenario_display = scenario_display[['Oil_Change_%', 'Predicted_Return_%', 'Predicted_Price', 'Price_Change']]\n",
    "print(scenario_display.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f72f2eb",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Analysis Conclusions\n",
    "\n",
    "### ðŸ“Š Key Findings\n",
    "\n",
    "1. **Model Performance**: \n",
    "   - All models showed negative RÂ² scores, indicating that predicting 30-day Nifty returns based on oil price features is challenging\n",
    "   - Linear models (Linear Regression, Ridge) performed best with ~63% directional accuracy\n",
    "   - Tree-based models (Random Forest, XGBoost) showed overfitting tendencies\n",
    "\n",
    "2. **Feature Insights**:\n",
    "   - Rolling averages and momentum indicators showed highest correlations with target\n",
    "   - Lagged features had minimal predictive power\n",
    "   - Low overall correlations suggest weak direct relationships\n",
    "\n",
    "3. **Scenario Analysis**:\n",
    "   - The model predictions show counterintuitive relationships (oil price decreases leading to higher Nifty returns)\n",
    "   - This suggests the need for more sophisticated feature engineering or different modeling approaches\n",
    "\n",
    "### ðŸ” Limitations and Future Work\n",
    "\n",
    "1. **Data Limitations**:\n",
    "   - Fixed USD/INR conversion rate oversimplifies currency impact\n",
    "   - Missing macroeconomic indicators (inflation, interest rates, etc.)\n",
    "   - No consideration of market sentiment or news events\n",
    "\n",
    "2. **Model Improvements**:\n",
    "   - Try time series models (LSTM, ARIMA) instead of traditional ML\n",
    "   - Include more economic indicators and technical analysis features\n",
    "   - Consider non-linear transformations and interaction terms\n",
    "\n",
    "3. **Analysis Enhancements**:\n",
    "   - Shorter prediction horizons (1-day, 7-day returns)\n",
    "   - Sector-specific analysis instead of broad market index\n",
    "   - Include global market indicators and geopolitical factors\n",
    "\n",
    "### ðŸ’¡ Recommendations\n",
    "\n",
    "1. **For Traders/Investors**:\n",
    "   - Oil price changes alone are insufficient for predicting Nifty movements\n",
    "   - Combine with other technical and fundamental indicators\n",
    "   - Consider shorter-term predictions with higher confidence\n",
    "\n",
    "2. **For Further Research**:\n",
    "   - Explore regime-switching models for different market conditions\n",
    "   - Investigate sector-wise impact (energy, transportation, chemicals)\n",
    "   - Include real-time USD/INR exchange rates and other macroeconomic factors\n",
    "\n",
    "### ðŸ› ï¸ Technical Notes\n",
    "\n",
    "- Dataset: 3,605 records from 2010-2025\n",
    "- Features: 15 engineered features from oil price and Nifty data\n",
    "- Best Model: Linear Regression with 63.5% directional accuracy\n",
    "- Prediction Horizon: 30-day forward returns\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This analysis demonstrates the complexity of financial market prediction and the importance of comprehensive feature engineering and model validation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
